---
title: "How long before superintelligence?"
date: 2015-03-24T01:02:08
lastmod: 2015-03-24T01:02:08
draft: true
categories: ["Philosophy"]
---

In 1997, Nick Bostrom wrote a paper entitled '[How long before superintelligence?](http://www.nickbostrom.com/superintelligence.html)', which he published the following year in the *International Journal of Futures Studies*. The abstract reads as follows:
> This paper outlines the case for believing that we will have superhuman artificial intelligence within the first third of the next century. It looks at different estimates of the processing power of the human brain; how long it will take until computer hardware achieve a similar performance; ways of creating the software through bottom-up approaches like the one used by biological brains; how difficult it will be for neuroscience figure out enough about how brains work to make this approach work; and how fast we can expect superintelligence to be developed once there is human-level artificial intelligence.

After publication, the paper was subject to a series of revisions, each consisting of an additional 'Postscript' being appended to the online version available at Bostrom's personal website.

The tone of the initial two postscripts is optimistic. The first one states that "[t]he U.S. Department of Energy has ordered a new supercomputer from IBM, to be installed in the Lawrence Livermore National Laboratory in the year 2000... This development is in accordance with Moore's law, or possibly slightly more rapid than an extrapolation would have predicted." The second one, written two years later, answers the question "Is progress still on schedule?" as follows: "In fact, things seem to be moving somewhat faster than expected, at least on the hardware front."

With the third postscript, from 2005, the tone changes. We read that "No dramatic breakthrough in general artificial intelligence seems to have occurred in recent years" (though "[n]euroscience and neuromorphic engineering" are said to be "proceeding at a rapid clip"). Similarly, the fourth and final postscript notes that, while "[t]here seems to be somewhat more interest now in artificial general intelligence (AGI) research than there was a few years ago... it appears that as yet no major breakthrough has occurred."

The final postscript also includes an interesting disclaimer:
> I should clarify what I meant when in the abstract I said I would "outline the case for believing that we will have superhuman artificial intelligence within the first third of the next [i.e. the this] century". I chose the word "case" deliberately: In particular, by outlinining "the case for", I did not mean to deny that one could also outline a case against. In fact, I would all-things-considered assign less than a 50% probability to superintelligence being developed by 2033. I do think there is great uncertainty about whether and when it might happen, and that one should take seriously the possibility that it might happen by then, because of the kinds of consideration outlined in this paper.

I find Bostrom's "clarification" hard to take seriously. The *Cambridge Dictionary of American Idioms* defines the expression *to make a case for something* as *to explain why something should be done* or *to explain that something is true*. Even when it is used in the strictly legal sense of *to argue for*, the conversational implicature of an utterance or statement containing that expression, barring special contexts, is that the person making the case for something believes this proposition to be more plausible than its contradictory. Someone reading books such as [*The Case for Christ*](http://www.amazon.com/Case-Christ-Journalists-Personal-Investigation-ebook/dp/B000FC2KEM/) or *[The Case for Marriage](http://www.amazon.com/Case-Marriage-Married-Healthier-Financially-ebook/dp/B000FC1H6O)* would be quite disappointed to learn that their authors in fact believe that an equally strong case could be made for atheism or cohabitation.

There is, moreover, independent evidence that Bostrom did not, in fact, assigned an "all-things-considered... less than a 50% probability to superintelligence being developed by 2033." The same year in which he wrote 'How long before superintelligence?', Bostrom wrote another paper, 'Predictions from philosophy'. The paper, which was never published but is available online, contains the following sentence:
> In my opinion there is more than 50% chance that superintelligence will be created within 40 years, possibly much sooner.

To me, it seems implausible that, back in 1997, Bostrom believed both that it was less than 50% likely that superintelligence would be developed by 2033, and that it was more than 50% likely that it would be developed by 2037, *and possibly much sooner*.

In light of these considerations, and the general evidence concerning [how people react when their predictions are proven wrong](http://www.slate.com/articles/health_and_science/science/2011/05/prophecy_fail.html), the following alternative explanation strikes me as far more plausible. At the time when he wrote the paper, Bostrom did in fact believe that superintelligence would probably be developed by 2033. Later, as it became increasingly less likely that this would happen, he reinterpreted parts of the original paper in such a way that his prediction would no longer be falsified by superintelligence's not being developed by that date. This reinterpretation was not a calculated attempt on Bostrom's part to deceive his audience; rather, it was an instance of self-deception, whereby Bostrom himself came to sincerely believe that his original credence wasn't as high as it really had been.

This post is not ultimately motivated by a desire to criticize Bostrom, a thinker for whom I have the utmost respect and admiration.  Rather, the post is an attempt to illustrate how even the most truth-oriented individuals can succumb to biases which lesser minds can detect with relative ease. If rosy retrospection and hindsight bias can affect Bostrom's thinking in the manner described here, consider how much damage these and other biases are causing in your own belief network.
